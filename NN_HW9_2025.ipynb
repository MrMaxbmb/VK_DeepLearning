{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": true,
        "id": "rwlxK5AYASaT"
      },
      "outputs": [],
      "source": [
        "# Данный ноутбук использовал окружение google-colab\n",
        "# %pip install catboost fasttext -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ошибка в формуле Wordpiece на семинаре\n",
        "\n",
        "$$\n",
        "\\text{score} = \\frac{\\text{freq\\_of\\_pair}}{\\text{freq\\_of\\_first\\_element}\\times \\text{freq\\_of\\_second\\_element}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```bash\n",
        "conda create -n hw9 python=3.10 -y\n",
        "\n",
        "conda activate hw9\n",
        "\n",
        "conda list\n",
        "\n",
        "conda install numpy matplotlib pandas scipy jupyter notebook ipykernel -y\n",
        "\n",
        "python -m ipykernel install --user --name=hw9_env --display-name=\"Python3.10 (hw9_env)\"\n",
        "\n",
        "which python\n",
        "\n",
        "nvidia-smi\n",
        "\n",
        "pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
        "\n",
        "python -c \"import torch; print(f'PyTorch: {torch.__version__}'); print(f'CUDA: {torch.cuda.is_available()}')\"\n",
        "\n",
        "pip3 install transformers fasttext datasets \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xRUXhCVUzur"
      },
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(52)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "outputs": [],
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "outputs": [],
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Тогда, переопределяем"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_vocab(texts: List[str], top_k: int = None, min_freq: int = 1) -> Tuple[List[str], Dict[str,int]]:\n",
        "    cnt = Counter()\n",
        "    for t in texts:\n",
        "        cnt.update(normalize_pretokenize_text(t))\n",
        "    items = [(w,f) for w,f in cnt.items() if f >= min_freq]\n",
        "    items.sort(key=lambda x: -x[1])\n",
        "    if top_k:\n",
        "        items = items[:top_k]\n",
        "    vocab = [w for w,_ in items]\n",
        "    vocab_index = {w:i for i,w in enumerate(vocab)}\n",
        "    return vocab, vocab_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['and',\n",
              " 'are',\n",
              " 'brown',\n",
              " 'dog',\n",
              " 'dogs',\n",
              " 'fox',\n",
              " 'foxes',\n",
              " 'jump',\n",
              " 'jumps',\n",
              " 'lazy',\n",
              " 'never',\n",
              " 'over',\n",
              " 'quick',\n",
              " 'quickly',\n",
              " 'the']"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'and': 0,\n",
              " 'are': 1,\n",
              " 'brown': 2,\n",
              " 'dog': 3,\n",
              " 'dogs': 4,\n",
              " 'fox': 5,\n",
              " 'foxes': 6,\n",
              " 'jump': 7,\n",
              " 'jumps': 8,\n",
              " 'lazy': 9,\n",
              " 'never': 10,\n",
              " 'over': 11,\n",
              " 'quick': 12,\n",
              " 'quickly': 13,\n",
              " 'the': 14}"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eemkFZ1tVLw4"
      },
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "outputs": [],
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[List[int]]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    vocab_size = len(vocab)\n",
        "    one_hot_vectors = []\n",
        "\n",
        "    for word in words:\n",
        "        vector = [0] * vocab_size\n",
        "        \n",
        "        if word in vocab_index:\n",
        "            idx = vocab_index[word]\n",
        "            vector[idx] = 1\n",
        "        \n",
        "        one_hot_vectors.append(vector)\n",
        "    \n",
        "    return one_hot_vectors\n",
        "\n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result[0]) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"the quick brown fox\"\n",
        "one_hot_vectorization(text, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"the quick brown fox abc and fox\"\n",
        "one_hot_vectorization(text, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Q2-LJcmbTe04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Придётся переопределить"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[int]:\n",
        "    words = set(normalize_pretokenize_text(text))\n",
        "    vector = [0] * len(vocab)\n",
        "    for w in words:\n",
        "        if w in vocab_index:\n",
        "            vector[vocab_index[w]] = 1\n",
        "    return vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"the quick brown fox\"\n",
        "one_hot_vectorization(text, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"the quick fox abc brown fox\"\n",
        "one_hot_vectorization(text, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__Минусы:__\n",
        "\n",
        "* Высокая размерность \n",
        "* Нет информации о семантической близости слов\n",
        "* Разреженные векторы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAF8IOYMVT3s"
      },
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__BoW - мешок слов:__\n",
        "\n",
        "* Учитывается частота каждого слова\n",
        "* Не учитывается порядок слов\n",
        "* Результат - словарь {слово: количество_вхождений}\n",
        "\n",
        "__Отличие от One-Hot:__\n",
        "\n",
        "* One-Hot: 0 или 1 (бинарное представление) для каждого слова\n",
        "* BoW: подсчёт частот (может быть > 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "outputs": [],
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    # Не зря же в начале импортировали Counter\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    return dict(Counter(words))\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ScFuXh_9TtJm"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'the': 2, 'quick': 1, 'brown': 3}"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = \"the the quick brown brown brown\"\n",
        "bag_of_words_vectorization(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bad-of-Words test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_bag_of_words_vectorization()  # Добавить ()text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6LblWJfX2kr"
      },
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**TF-IDF (Term Frequency - Inverse Document Frequency)** - это статистическая мера важности слова в документе относительно коллекции документов (корпуса).\n",
        "\n",
        "**Формулы:**\n",
        "\n",
        "$$\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)$$\n",
        "\n",
        "$$\\text{TF}(t, d) = \\frac{\\text{count}(t \\text{ in } d)}{\\text{total words in } d}$$\n",
        "\n",
        "$$\\text{IDF}(t) = \\log \\frac{N}{1 + |\\{d \\in D : t \\in d\\}|}$$\n",
        "\n",
        "Где:\n",
        "- `t` - термин (слово)\n",
        "- `d` - документ\n",
        "- `N` - общее количество документов\n",
        "- `|{d ∈ D : t ∈ d}|` - количество документов, содержащих термин `t`. Опционально - Лаплас сглаживание\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "outputs": [],
      "source": [
        "def tf_idf_vectorization(\n",
        "        text: str,\n",
        "        corpus: List[str] = None,\n",
        "        vocab: List[str] = None,\n",
        "        vocab_index: Dict[str, int] = None  # не исплользуем\n",
        ") -> List[float]:\n",
        "    \n",
        "    words = normalize_pretokenize_text(text)\n",
        "    \n",
        "    word_count = Counter(words)\n",
        "    total_words = len(words)\n",
        "    tf = {}\n",
        "    for word in vocab:\n",
        "        tf[word] = word_count.get(word, 0) / total_words if total_words > 0 else 0\n",
        "    \n",
        "    # Inverse Document Frequency\n",
        "    N = len(corpus)\n",
        "    idf = {}\n",
        "    \n",
        "    for word in vocab:\n",
        "        # Считаем в скольких документах встречается слово\n",
        "        doc_count = 0\n",
        "        for doc in corpus:\n",
        "            doc_words = normalize_pretokenize_text(doc)\n",
        "            if word in doc_words:\n",
        "                doc_count += 1\n",
        "        \n",
        "        # IDF = log(N / (1 + doc_count))\n",
        "        # +1 в знаменателе для избежания деления на 0 (Лаплас)\n",
        "        idf[word] = math.log(N / (1 + doc_count))\n",
        "    \n",
        "    # Вычисляем TF-IDF\n",
        "    tfidf_vector = []\n",
        "    for word in vocab:\n",
        "        tfidf_value = tf[word] * idf[word]\n",
        "        tfidf_vector.append(tfidf_value)\n",
        "    \n",
        "    return tfidf_vector\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "GKIyS724T0XH"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0f9FZCrX5_s"
      },
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "\n",
        "$$\n",
        "PPMI(word, context) = max(0, PMI(word, context))\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "PMI(word, context) = log \\frac{P(word, context)}  {P(word) \\times P(context)} = log \\frac{N(word, context)\\times|(word, context)|}{N(word) \\times N(context)}\n",
        "$$\n",
        "\n",
        "<br>\n",
        "\n",
        "Где:\n",
        "- `N(word, context)` - число вхождений слова `word` в окно `context` (размер окна - гиперпараметр)\n",
        "- `|(word, context)|` — сколько раз встречается context\n",
        "\n",
        "Выводы:\n",
        "\n",
        "- Если слова \"king\" и \"queen\" часто встречаются рядом -> высокий PMI\n",
        "- Если \"the\" встречается со всеми словами -> низкий PMI\n",
        "- PPMI обрезает отрицательные значения <- (max(0, PMI))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "outputs": [],
      "source": [
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "    \n",
        "    words = normalize_pretokenize_text(text)\n",
        "    \n",
        "    word_counts = Counter()  # Сколько раз встречается каждое слово\n",
        "    context_counts = Counter()  # Сколько раз слово является контекстом\n",
        "    pair_counts = defaultdict(int)  # Сколько раз пара (word, context) встречается\n",
        "    total_pairs = 0\n",
        "    \n",
        "    for doc in corpus:\n",
        "        doc_words = normalize_pretokenize_text(doc)\n",
        "        \n",
        "        # Обновляем счётчик слов\n",
        "        word_counts.update(doc_words)\n",
        "        \n",
        "        # Для каждого слова считаем контекст в окне\n",
        "        for i, target_word in enumerate(doc_words):\n",
        "            # Левое окно: [i-window_size, i)\n",
        "            # Правое окно: (i, i+window_size]\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(len(doc_words), i + window_size + 1)\n",
        "            \n",
        "            for j in range(start, end):\n",
        "                if i != j:  # Не берём само слово\n",
        "                    context_word = doc_words[j]\n",
        "                    pair_counts[(target_word, context_word)] += 1\n",
        "                    context_counts[context_word] += 1\n",
        "                    total_pairs += 1\n",
        "    \n",
        "    # Вычисляем PPMI для слов из текущего текста\n",
        "    ppmi_vector = []\n",
        "    \n",
        "    for context_word in vocab:\n",
        "        # Собираем PMI для всех слов в тексте относительно context_word\n",
        "        pmi_sum = 0.0\n",
        "        count = 0\n",
        "        \n",
        "        for target_word in words:\n",
        "            if target_word in word_counts:\n",
        "                # Количество пар (target_word, context_word)\n",
        "                n_word_context = pair_counts.get((target_word, context_word), 0)\n",
        "                \n",
        "                if n_word_context > 0:\n",
        "\n",
        "                    p_word_context = n_word_context / total_pairs\n",
        "                    \n",
        "                    p_word = word_counts[target_word] / sum(word_counts.values())\n",
        "                    \n",
        "                    p_context = context_counts[context_word] / sum(context_counts.values())\n",
        "                    \n",
        "                    # PMI = log(P(word, context) / (P(word) * P(context)))\n",
        "                    pmi = math.log(p_word_context / (p_word * p_context))\n",
        "                    \n",
        "                    ppmi = max(0, pmi)\n",
        "                    \n",
        "                    pmi_sum += ppmi\n",
        "                    count += 1\n",
        "        \n",
        "        # Усредняем PPMI по всем словам в тексте\n",
        "        avg_ppmi = pmi_sum / count if count > 0 else 0.0\n",
        "        ppmi_vector.append(avg_ppmi)\n",
        "    \n",
        "    return ppmi_vector\n",
        "\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HgHmNZy75XFV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPMI test PASSED\n"
          ]
        }
      ],
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK29va3PBH_8"
      },
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "__FastText__\n",
        "\n",
        "* Основан на Word2Vec, но учитывает подслова (subwords)\n",
        "* Может работать с незнакомыми словами (out-of-vocabulary)\n",
        "\n",
        "\n",
        "__BERT (Bidirectional Encoder Representations from Transformers)__\n",
        "* Основан на архитектуре Transformer\n",
        "* Контекстуальные эмбеддинги: одно слово имеет __разные векторы в разных контекстах__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "outputs": [],
      "source": [
        "def get_fasttext_embeddings(\n",
        "        text: str, \n",
        "        model_path: str = None, \n",
        "        model: any = None\n",
        ") -> List[np.ndarray]:\n",
        "    \n",
        "    # Загружаем модель, если она не передана\n",
        "    if model is None:\n",
        "        if model_path is None:\n",
        "            # Скачиваем предобученную модель (английский язык)\n",
        "            fasttext.util.download_model('en', if_exists='ignore')\n",
        "            model_path = 'cc.en.300.bin'\n",
        "        \n",
        "        model = fasttext.load_model(model_path)\n",
        "    \n",
        "    words = normalize_pretokenize_text(text)\n",
        "    \n",
        "    embeddings = []\n",
        "    for word in words:\n",
        "        # get_word_vector возвращает numpy array размером (300,)\n",
        "        word_embedding = model.get_word_vector(word)\n",
        "        embeddings.append(word_embedding)\n",
        "    \n",
        "    return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1.26 s, sys: 14.2 s, total: 15.4 s\n",
            "Wall time: 16.4 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 4, 300)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "sh = np.array([get_fasttext_embeddings('I am ml engineer')])\n",
        "sh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pool_method: str = 'cls'`  \n",
        "- mean: для семантического поиска, сравнения документов или max - для выделения ключевых признаков, semantic similarity, поиск\n",
        "- max: вытягивает самые «сильные» активации, иногда полезно для извлечения маркерных признаков (ключевых слов, эмоций)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "outputs": [],
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls' \n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Возвращает один эмбеддинг текста на основе выбранного метода пуллинга.\n",
        "    \"\"\"\n",
        "\n",
        "    # Загружаем токенизатор и модель\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "    model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    \n",
        "    # Переводим модель в режим inference (отключаем dropout)\n",
        "    model.eval()    \n",
        "    \n",
        "    # см ниже\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',  # возвращает PyTorch тензоры\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512  # Лимит длина для BERT\n",
        "    )\n",
        "\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    # print(inputs['input_ids'].shape)  \n",
        "    # print(inputs['attention_mask'].shape) \n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    \n",
        "    # Извлекаем эмбеддинги\n",
        "    last_hidden_state = outputs.last_hidden_state  # (1, seq_len, 768) # hidden_size = 768 для bert-base-uncased\n",
        "    \n",
        "    # Применяем pooling\n",
        "    if pool_method == 'cls':\n",
        "        # Берём эмбеддинг токена [CLS] (первый токен)\n",
        "        embedding = last_hidden_state[0, 0, :].detach().cpu().numpy()  # (768,)\n",
        "        \n",
        "    else:\n",
        "        raise ValueError(f\"Unk: {pool_method}\")\n",
        "    \n",
        "    \n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 219 ms, sys: 431 ms, total: 650 ms\n",
            "Wall time: 2.23 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1, 768)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "sh = np.array([get_bert_embeddings('I am ml engineer')])\n",
        "sh.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`inputs = tokenizer():`\n",
        "\n",
        "- tokenizer разбивает текст на WordPiece токены, добавляет специальные [CLS] и [SEP]\n",
        "- Формирует input_ids (индексы токенов) и attention_mask (1 — реальный токен, 0 — паддинг)\n",
        "\n",
        "\n",
        "В стандартном BertModel (из transformers) кроме last_hidden_state возвращается (если не выключено) ещё pooler_output:\n",
        "\n",
        "* Берётся вектор CLS: h_cls = last_hidden_state[:, 0] (shape: (batch, 768))\n",
        "* Прогоняется через линейный слой + tanh:\n",
        "* pooler_output = tanh(W * h_cls + b)\n",
        "* Обучалось в pretraining для задачи Next Sentence Prediction (NSP)\n",
        "* Этот слой заточен под NSP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Оптимизация get_bert_embeddings (кэширование модели + избегаем повторной загрузки)\n",
        "\n",
        "Проблема: сейчас модель и токенизатор грузятся при каждом вызове огромные накладные расходы (IO + cuda init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "_DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "_BERT_TOKENIZER = None\n",
        "_BERT_MODEL = None\n",
        "\n",
        "def init_bert(model_name: str = 'bert-base-uncased'):\n",
        "    global _BERT_TOKENIZER, _BERT_MODEL\n",
        "    if _BERT_MODEL is None:\n",
        "        _BERT_TOKENIZER = BertTokenizer.from_pretrained(model_name)\n",
        "        _BERT_MODEL = BertModel.from_pretrained(model_name)\n",
        "        _BERT_MODEL.to(_DEVICE).eval()\n",
        "\n",
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "    init_bert()\n",
        "    enc = _BERT_TOKENIZER(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "    enc = {k: v.to(_DEVICE) for k, v in enc.items()}\n",
        "    with torch.no_grad():\n",
        "        out = _BERT_MODEL(**enc)\n",
        "    hidden = out.last_hidden_state  # (1, L, 768)\n",
        "    if pool_method == 'cls':\n",
        "        emb = hidden[0, 0]\n",
        "\n",
        "    else:\n",
        "        raise ValueError(pool_method)\n",
        "    return emb.detach().cpu().numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Теперь:\n",
        "\n",
        "* `init_bert` обеспечивает однократную загрузку\n",
        "* Сокращает время векторизации корпуса в разы\n",
        "\n",
        "Аналогично для Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "_FASTTEXT_MODEL = None\n",
        "\n",
        "def init_fasttext(lang: str = 'en'):\n",
        "    global _FASTTEXT_MODEL\n",
        "    if _FASTTEXT_MODEL is None:\n",
        "        fasttext.util.download_model(lang, if_exists='ignore')\n",
        "        _FASTTEXT_MODEL = fasttext.load_model(f'cc.{lang}.300.bin')\n",
        "\n",
        "def get_fasttext_mean_embedding(text: str) -> List[float]:\n",
        "    init_fasttext()\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    if not words:\n",
        "        return [0.0]*300\n",
        "    vecs = [_FASTTEXT_MODEL.get_word_vector(w) for w in words]\n",
        "    return np.mean(vecs, axis=0).tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_KoKolrD49R"
      },
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Единая длина признаков внутри одного метода\n",
        "\n",
        "CatBoost (и большинство ML-моделей) требует одинаковую длину feature в каждой строке\n",
        "\n",
        "\n",
        "Поэтому:\n",
        "- строим vocab только по train, затем при векторизации test использем тот же vocab (а неизвестные слова игнор)\n",
        "- PPMI/TF-IDF  важна консистентность корпуса (для честности лучше вычислять IDF только на train, а test использовать предрасчитанные значения)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_idf(corpus: List[str], vocab: List[str]) -> Dict[str,float]:\n",
        "    N = len(corpus)\n",
        "    df = {w:0 for w in vocab}\n",
        "    for doc in corpus:\n",
        "        seen = set(normalize_pretokenize_text(doc))\n",
        "        for w in seen:\n",
        "            if w in df:\n",
        "                df[w] += 1\n",
        "    idf = {w: math.log(N / (1 + df[w])) for w in vocab}\n",
        "    return idf\n",
        "\n",
        "def tf_idf_vector(text: str, vocab: List[str], idf: Dict[str,float]) -> List[float]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    cnt = Counter(words)\n",
        "    total = len(words) or 1\n",
        "    vec = []\n",
        "    for w in vocab:\n",
        "        tf = cnt.get(w,0)/total\n",
        "        vec.append(tf * idf[w])\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_ppmi_stats(corpus: List[str], vocab: List[str], window_size: int = 2):\n",
        "    pair_counts = Counter()\n",
        "    word_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "    for doc in corpus:\n",
        "        doc_words = normalize_pretokenize_text(doc)\n",
        "        word_counts.update(doc_words)\n",
        "        for i,w in enumerate(doc_words):\n",
        "            start = max(0, i-window_size)\n",
        "            end = min(len(doc_words), i+window_size+1)\n",
        "            for j in range(start,end):\n",
        "                if i==j: continue\n",
        "                c = doc_words[j]\n",
        "                pair_counts[(w,c)] += 1\n",
        "                context_counts[c] += 1\n",
        "    total_pairs = sum(pair_counts.values()) or 1\n",
        "    return pair_counts, word_counts, context_counts, total_pairs\n",
        "\n",
        "def ppmi_vector_fast(text: str,\n",
        "                     vocab: List[str],\n",
        "                     pair_counts: Counter,\n",
        "                     word_counts: Counter,\n",
        "                     context_counts: Counter,\n",
        "                     total_pairs: int) -> List[float]:\n",
        "    words = set(normalize_pretokenize_text(text))\n",
        "    sum_words = sum(word_counts.values()) or 1\n",
        "    sum_context = sum(context_counts.values()) or 1\n",
        "    vec=[]\n",
        "    for ctx in vocab:\n",
        "        pmi_sum=0.0\n",
        "        cnt=0\n",
        "        for w in words:\n",
        "            n = pair_counts.get((w,ctx),0)\n",
        "            if n==0: continue\n",
        "            p_wc = n/total_pairs\n",
        "            p_w = word_counts[w]/sum_words\n",
        "            p_c = context_counts[ctx]/sum_context\n",
        "            pmi = math.log(p_wc/(p_w*p_c))\n",
        "            pmi_sum += max(0,pmi)\n",
        "            cnt +=1\n",
        "        vec.append(pmi_sum/cnt if cnt else 0.0)\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "vectorize_dataset тоже обновлю\n",
        "\n",
        "как минимум отсутсвует `return vocab, vectorized_data, labels`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "outputs": [],
      "source": [
        "def vectorize_dataset(\n",
        "    dataset_name: str = \"imdb\",\n",
        "    vectorizer_type: str = \"bow\",\n",
        "    split: str = \"train\",\n",
        "    sample_size: int = 2500,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    idf: Dict[str, float] = None,\n",
        "    ppmi_stats: Tuple = None,\n",
        "    top_k: int = 5000\n",
        ") -> Tuple[List[str], Dict[str, int], Any, List[List[float]], List[int]]:\n",
        "    \"\"\"\n",
        "    Возвращает:\n",
        "      vocab, vocab_index,\n",
        "      stats (idf для tfidf или (pair_counts, word_counts, context_counts, total_pairs) для ppmi),\n",
        "      vectors, labels\n",
        "    \"\"\"\n",
        "    dataset = datasets.load_dataset(\"imdb\", split=split).shuffle(seed=42)  # CatBoost не обучается, когда y содержит единственное уникальное значение\n",
        "    if sample_size:\n",
        "        dataset = dataset.select(range(sample_size))\n",
        "\n",
        "    texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "    labels = [item['label'] for item in dataset if 'label' in item]\n",
        "\n",
        "    # Строим vocab только если не передан (т.е. train)\n",
        "    if vocab is None or vocab_index is None:\n",
        "        vocab, vocab_index = build_vocab(texts, top_k=top_k)\n",
        "\n",
        "    # Предрасчёт статистик только на train\n",
        "    if split == \"train\":\n",
        "        if vectorizer_type == \"tfidf\" and idf is None:\n",
        "            idf = compute_idf(texts, vocab)\n",
        "        if vectorizer_type == \"ppmi\" and ppmi_stats is None:\n",
        "            ppmi_stats = compute_ppmi_stats(texts, vocab)\n",
        "\n",
        "    vectors = []\n",
        "    for text in texts:\n",
        "        if vectorizer_type == \"one_hot\":\n",
        "            vectors.append(one_hot_vectorization(text, vocab, vocab_index))\n",
        "        elif vectorizer_type == \"bow\":\n",
        "            bow_dict = bag_of_words_vectorization(text)\n",
        "            vectors.append([bow_dict.get(w, 0) for w in vocab])\n",
        "        elif vectorizer_type == \"tfidf\":\n",
        "            if idf is None:\n",
        "                raise ValueError(\"idf must be provided for test split\")\n",
        "            vectors.append(tf_idf_vector(text, vocab, idf))\n",
        "        elif vectorizer_type == \"ppmi\":\n",
        "            if ppmi_stats is None:\n",
        "                raise ValueError(\"ppmi_stats must be provided for test split\")\n",
        "            vectors.append(ppmi_vector_fast(text, vocab, *ppmi_stats))\n",
        "        elif vectorizer_type == \"fasttext\":\n",
        "            vectors.append(get_fasttext_mean_embedding(text))\n",
        "        elif vectorizer_type == \"bert\":\n",
        "            emb = get_bert_embeddings(text)\n",
        "            vectors.append(emb.tolist())\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown vectorizer_type: {vectorizer_type}\")\n",
        "\n",
        "    # Проверка одинаковой длины\n",
        "    if vectors:\n",
        "        base_len = len(vectors[0])\n",
        "        for v in vectors:\n",
        "            assert len(v) == base_len, \"Feature length mismatch\"\n",
        "\n",
        "    stats = idf if vectorizer_type == \"tfidf\" else ppmi_stats\n",
        "    return vocab, vocab_index, stats, vectors, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. после теста переопределю `one_hot_vectorization`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "outputs": [],
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "def train(\n",
        "    embeddings_method: str = \"bow\",\n",
        "    sample_size: int = 1500,\n",
        "    random_state: int = 42\n",
        "):\n",
        "    print(f\"\\n=== {embeddings_method.upper()} ===\")\n",
        "    vocab, vocab_index, stats, X_train_raw, y_train = vectorize_dataset(\n",
        "        \"imdb\",\n",
        "        embeddings_method,\n",
        "        \"train\",\n",
        "        sample_size=sample_size\n",
        "    )\n",
        "\n",
        "    # Подставляем нужные статистики для test\n",
        "    idf = stats if embeddings_method == \"tfidf\" else None\n",
        "    ppmi_stats = stats if embeddings_method == \"ppmi\" else None\n",
        "\n",
        "    _, _, _, X_test_raw, y_test = vectorize_dataset(\n",
        "        \"imdb\",\n",
        "        embeddings_method,\n",
        "        \"test\",\n",
        "        sample_size=sample_size,\n",
        "        vocab=vocab,\n",
        "        vocab_index=vocab_index,\n",
        "        idf=idf,\n",
        "        ppmi_stats=ppmi_stats\n",
        "    )\n",
        "\n",
        "    X_train_raw = np.array(X_train_raw, dtype=np.float32)\n",
        "    y_train = np.array(y_train)\n",
        "    X_test_raw = np.array(X_test_raw, dtype=np.float32)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "        X_train_raw,\n",
        "        y_train,\n",
        "        test_size=0.2,\n",
        "        random_state=random_state,\n",
        "        stratify=y_train\n",
        "    )\n",
        "\n",
        "    model = CatBoostClassifier(\n",
        "        iterations=250,\n",
        "        depth=4,\n",
        "        learning_rate=0.1,\n",
        "        loss_function=\"Logloss\",\n",
        "        verbose=50,\n",
        "        random_seed=random_state\n",
        "    )\n",
        "    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True)\n",
        "\n",
        "    val_pred = model.predict(X_val)\n",
        "    test_pred = model.predict(X_test_raw)\n",
        "\n",
        "    val_acc = accuracy_score(y_val, val_pred)\n",
        "    val_f1 = f1_score(y_val, val_pred)\n",
        "    test_acc = accuracy_score(y_test, test_pred)\n",
        "    test_f1 = f1_score(y_test, test_pred)\n",
        "\n",
        "    print(f\"Val: acc={val_acc:.4f} f1={val_f1:.4f}\")\n",
        "    print(f\"Test: acc={test_acc:.4f} f1={test_f1:.4f}\")\n",
        "\n",
        "    return dict(val_acc=val_acc, val_f1=val_f1, test_acc=test_acc, test_f1=test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "collapsed": true,
        "id": "naMqAkjqFHAe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== BOW ===\n",
            "0:\tlearn: 0.6808517\ttest: 0.6876472\tbest: 0.6876472 (0)\ttotal: 15.8ms\tremaining: 3.94s\n",
            "50:\tlearn: 0.4210989\ttest: 0.5632028\tbest: 0.5617851 (49)\ttotal: 222ms\tremaining: 868ms\n",
            "100:\tlearn: 0.2666398\ttest: 0.5254912\tbest: 0.5240647 (94)\ttotal: 380ms\tremaining: 560ms\n",
            "150:\tlearn: 0.1845556\ttest: 0.5176552\tbest: 0.5137885 (145)\ttotal: 539ms\tremaining: 353ms\n",
            "200:\tlearn: 0.1321614\ttest: 0.5112358\tbest: 0.5112358 (200)\ttotal: 687ms\tremaining: 167ms\n",
            "249:\tlearn: 0.1026263\ttest: 0.5112262\tbest: 0.5091709 (237)\ttotal: 854ms\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5091709188\n",
            "bestIteration = 237\n",
            "\n",
            "Shrink model to first 238 iterations.\n",
            "Val: acc=0.7500 f1=0.7531\n",
            "Test: acc=0.7875 f1=0.7942\n",
            "\n",
            "=== ONE_HOT ===\n",
            "0:\tlearn: 0.6764145\ttest: 0.6883271\tbest: 0.6883271 (0)\ttotal: 10.5ms\tremaining: 2.61s\n",
            "50:\tlearn: 0.4119559\ttest: 0.5670598\tbest: 0.5670598 (50)\ttotal: 183ms\tremaining: 713ms\n",
            "100:\tlearn: 0.2752558\ttest: 0.5352554\tbest: 0.5346894 (97)\ttotal: 413ms\tremaining: 609ms\n",
            "150:\tlearn: 0.1889934\ttest: 0.5230152\tbest: 0.5169677 (144)\ttotal: 606ms\tremaining: 397ms\n",
            "200:\tlearn: 0.1414757\ttest: 0.5226577\tbest: 0.5169677 (144)\ttotal: 779ms\tremaining: 190ms\n",
            "249:\tlearn: 0.1128514\ttest: 0.5205253\tbest: 0.5169677 (144)\ttotal: 952ms\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5169677012\n",
            "bestIteration = 144\n",
            "\n",
            "Shrink model to first 145 iterations.\n",
            "Val: acc=0.7250 f1=0.7215\n",
            "Test: acc=0.7925 f1=0.7985\n",
            "\n",
            "=== TFIDF ===\n",
            "0:\tlearn: 0.6775887\ttest: 0.6863161\tbest: 0.6863161 (0)\ttotal: 7.14ms\tremaining: 1.78s\n",
            "50:\tlearn: 0.3840791\ttest: 0.5724955\tbest: 0.5724955 (50)\ttotal: 261ms\tremaining: 1.02s\n",
            "100:\tlearn: 0.2311497\ttest: 0.5448249\tbest: 0.5442639 (86)\ttotal: 534ms\tremaining: 788ms\n",
            "150:\tlearn: 0.1327472\ttest: 0.5465228\tbest: 0.5313483 (121)\ttotal: 781ms\tremaining: 512ms\n",
            "200:\tlearn: 0.0847870\ttest: 0.5467528\tbest: 0.5313483 (121)\ttotal: 1s\tremaining: 244ms\n",
            "249:\tlearn: 0.0561502\ttest: 0.5453299\tbest: 0.5313483 (121)\ttotal: 1.23s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.531348298\n",
            "bestIteration = 121\n",
            "\n",
            "Shrink model to first 122 iterations.\n",
            "Val: acc=0.7188 f1=0.7239\n",
            "Test: acc=0.7750 f1=0.7789\n",
            "\n",
            "=== FASTTEXT ===\n",
            "0:\tlearn: 0.6660766\ttest: 0.6862491\tbest: 0.6862491 (0)\ttotal: 7.94ms\tremaining: 1.98s\n",
            "50:\tlearn: 0.3412058\ttest: 0.5916350\tbest: 0.5887810 (49)\ttotal: 208ms\tremaining: 813ms\n",
            "100:\tlearn: 0.1728572\ttest: 0.5820834\tbest: 0.5775765 (82)\ttotal: 369ms\tremaining: 545ms\n",
            "150:\tlearn: 0.0809101\ttest: 0.6171591\tbest: 0.5767176 (113)\ttotal: 525ms\tremaining: 344ms\n",
            "200:\tlearn: 0.0421997\ttest: 0.6459725\tbest: 0.5767176 (113)\ttotal: 684ms\tremaining: 167ms\n",
            "249:\tlearn: 0.0258640\ttest: 0.6758244\tbest: 0.5767176 (113)\ttotal: 832ms\tremaining: 0us\n",
            "\n",
            "bestTest = 0.5767175732\n",
            "bestIteration = 113\n",
            "\n",
            "Shrink model to first 114 iterations.\n",
            "Val: acc=0.7063 f1=0.7006\n",
            "Test: acc=0.7150 f1=0.7128\n",
            "\n",
            "=== BERT ===\n",
            "0:\tlearn: 0.6697437\ttest: 0.6738821\tbest: 0.6738821 (0)\ttotal: 9.66ms\tremaining: 2.41s\n",
            "50:\tlearn: 0.2777503\ttest: 0.4709599\tbest: 0.4709599 (50)\ttotal: 365ms\tremaining: 1.42s\n",
            "100:\tlearn: 0.1223990\ttest: 0.4228154\tbest: 0.4228154 (100)\ttotal: 769ms\tremaining: 1.13s\n",
            "150:\tlearn: 0.0512107\ttest: 0.4051767\tbest: 0.3985762 (141)\ttotal: 1.19s\tremaining: 781ms\n",
            "200:\tlearn: 0.0256343\ttest: 0.4256574\tbest: 0.3985762 (141)\ttotal: 1.54s\tremaining: 376ms\n",
            "249:\tlearn: 0.0143720\ttest: 0.4343240\tbest: 0.3985762 (141)\ttotal: 1.93s\tremaining: 0us\n",
            "\n",
            "bestTest = 0.3985762324\n",
            "bestIteration = 141\n",
            "\n",
            "Shrink model to first 142 iterations.\n",
            "Val: acc=0.8250 f1=0.8182\n",
            "Test: acc=0.7950 f1=0.7955\n",
            "{'bow': {'val_acc': 0.75, 'val_f1': 0.7530864197530864, 'test_acc': 0.7875, 'test_f1': 0.7941888619854721}, 'one_hot': {'val_acc': 0.725, 'val_f1': 0.7215189873417721, 'test_acc': 0.7925, 'test_f1': 0.7985436893203883}, 'tfidf': {'val_acc': 0.71875, 'val_f1': 0.7239263803680982, 'test_acc': 0.775, 'test_f1': 0.7788697788697788}, 'fasttext': {'val_acc': 0.70625, 'val_f1': 0.7006369426751592, 'test_acc': 0.715, 'test_f1': 0.7128463476070529}, 'bert': {'val_acc': 0.825, 'val_f1': 0.8181818181818182, 'test_acc': 0.795, 'test_f1': 0.7955112219451371}}\n"
          ]
        }
      ],
      "source": [
        "results = {}\n",
        "for m in [\"bow\", \"one_hot\", \"tfidf\", \"fasttext\", \"bert\"]:\n",
        "    results[m] = train(m, sample_size=800)\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Не ожидал что будут такие хорошие метрики даже с такой реализацией"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Более того, ohe не сильно хуже bert, но это скорее всего из-за маленткой выборки"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3.10 (hw9_env)",
      "language": "python",
      "name": "hw9_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

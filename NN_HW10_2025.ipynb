{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "conda create -n hw10 python=3.11 -y\n",
        "\n",
        "conda activate hw10\n",
        "\n",
        "pip3 install torch \n",
        "\n",
        "conda install numpy matplotlib pandas scipy jupyter notebook ipykernel -y\n",
        "\n",
        "pip install tqdm transformers\n",
        "\n",
        "python -m ipykernel install --user --name hw10 --display-name \"Python 3.11 (hw10)\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tjJocIf2SVzR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple  # намёк на использование)\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvgXC6YxSm12"
      },
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __Greedy Sampling (Жадный выбор)__\n",
        "Принцип работы:<br>\n",
        "На каждом шаге выбирается самый вероятный токен (слово)\n",
        "\n",
        "Плюсы:\n",
        "- Быстрый и простой\n",
        "- Детерминированный \n",
        "\n",
        "Минусы:\n",
        "- Предсказуемый и скучный текст\n",
        "- Может застрять в повторениях "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## __Random Sampling__\n",
        "Принцип работы:<br>\n",
        "Выбирается токен случайно, согласно распределению вероятностей модели\n",
        "\n",
        "\n",
        "Плюсы:\n",
        "- Разнообразный текст\n",
        "- Креативные результаты\n",
        "\n",
        "Минусы:\n",
        "- Может генерировать бессмыслицу\n",
        "- Недетерминированный (каждый раз разный результат)\n",
        "- Может выбрать очень маловероятные слова\n",
        "\n",
        "Улучшения: Temperature, Top-K, Top-P (об этом позже)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### __Beam Search__\n",
        "Принцип работы:<br>\n",
        "Держим несколько гипотез (лучей) одновременно и выбираем лучшую последовательность целиком.\n",
        "\n",
        "Плюсы:\n",
        "- Находит более оптимальные последовательности\n",
        "- Хорош для задач с умным / правильным ответом (перевод, суммаризация)\n",
        "- Детерминированный\n",
        "\n",
        "Минусы:\n",
        "- Медленнее в beam_size раз\n",
        "- Требует больше памяти"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "\n",
        "\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return torch.argmax(logits, dim=-1).item()\n",
        "    \n",
        "\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        # Преобразуем logits в вероятности через softmax\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Случайно выбираем токен согласно вероятностям\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "    \n",
        "        return next_token.item()\n",
        "    \n",
        "\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        # Токенизируем промпт\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "        # Инициализацию начинаем с одного луча\n",
        "        # beams хранит список кортежей: (последовательность_токенов, накопленный_score)\n",
        "        beams = [(input_ids, 0.0)]  # score в log-пространстве (начинаем с log(1) = 0)\n",
        "\n",
        "        # Генерируем токены до достижения max_length\n",
        "        for _ in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            # Для каждого луча генерируем продолжения\n",
        "            for seq, score in beams:\n",
        "                # Получаем предсказания модели\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model(seq)\n",
        "                    logits = outputs.logits[0, -1, :]  # берём logits последнего токена\n",
        "\n",
        "                # Преобразуем в log-вероятности\n",
        "                log_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "                # Берём top-k токенов с наибольшими вероятностями\n",
        "                topk_log_probs, topk_indices = torch.topk(log_probs, num_beams)\n",
        "\n",
        "                # Создаём новые кандидаты\n",
        "                for i in range(num_beams):\n",
        "                    next_token = topk_indices[i].unsqueeze(0).unsqueeze(0)\n",
        "                    new_seq = torch.cat([seq, next_token], dim=-1)\n",
        "                    new_score = score + topk_log_probs[i].item()\n",
        "\n",
        "                    all_candidates.append((new_seq, new_score))\n",
        "\n",
        "            # Сортируем всех кандидатов по score и берём лучшие num_beams\n",
        "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "            # Проверяем, все ли лучи закончились (встретили EOS токен)\n",
        "            if all(seq[0, -1].item() == self.tokenizer.eos_token_id for seq, _ in beams):\n",
        "                break\n",
        "            \n",
        "        # Возвращаем лучшую гипотезу\n",
        "        best_seq = beams[0][0]\n",
        "        return self.tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
        "    \n",
        "\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = 0) -> torch.Tensor:  # тут наверное int\n",
        "        if top_k <= 0:\n",
        "            return logits\n",
        "\n",
        "        # Получаем top_k значений\n",
        "        top_k = min(top_k, logits.size(-1))\n",
        "        topk_values, _ = torch.topk(logits, top_k)\n",
        "        min_value = topk_values[..., -1, None]\n",
        "\n",
        "        # Обнуляем все значения меньше минимального из top_k\n",
        "        return torch.where(\n",
        "            logits < min_value,\n",
        "            torch.full_like(logits, float('-inf')),\n",
        "            logits\n",
        "        )\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1) -> torch.Tensor:  # self перенести в функцию\n",
        "        if top_p >= 1.0:\n",
        "            return logits\n",
        "        \n",
        "        # Сортируем logits по убыванию\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        \n",
        "        # Преобразуем в вероятности\n",
        "        sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
        "        \n",
        "        # Считаем кумулятивную сумму вероятностей\n",
        "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "        \n",
        "        # Находим токены, которые нужно удалить (те, что после порога top_p)\n",
        "        # Сдвигаем на 1 вправо, чтобы включить первый токен, который превысил порог\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = False\n",
        "        \n",
        "        # Создаём маску для исходных индексов\n",
        "        indices_to_remove = torch.zeros_like(logits, dtype=torch.bool)\n",
        "        indices_to_remove[sorted_indices] = sorted_indices_to_remove\n",
        "        \n",
        "        # Обнуляем отфильтрованные токены\n",
        "        logits = logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "        \n",
        "        return logits\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "        # Beam search обрабатывается отдельно\n",
        "        if strategy == \"beam\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "        \n",
        "        # Для greedy и random используем autoregressive generation\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "        \n",
        "        for _ in range(max_length):\n",
        "            # Получаем предсказания модели\n",
        "            with torch.no_grad():\n",
        "                outputs = self.model(input_ids)\n",
        "                logits = outputs.logits[0, -1, :]  # logits последнего токена\n",
        "            \n",
        "            # Применяем фильтрацию (только для random sampling)\n",
        "            if strategy == \"random\":\n",
        "                logits = self.apply_temperature(logits, temperature)\n",
        "                logits = self._apply_top_k(logits, top_k)\n",
        "                logits = self._apply_top_p(logits, top_p)\n",
        "            \n",
        "            # Выбираем следующий токен\n",
        "            if strategy == \"greedy\":\n",
        "                next_token = self.greedy_sampling(logits)\n",
        "            elif strategy == \"random\":\n",
        "                next_token = self.random_sampling(logits)\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "            \n",
        "            # Добавляем токен к последовательности\n",
        "            input_ids = torch.cat([\n",
        "                input_ids,\n",
        "                torch.tensor([[next_token]])\n",
        "            ], dim=-1)\n",
        "            \n",
        "            # Останавливаемся при встрече EOS токена\n",
        "            if next_token == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "            \n",
        "        # Декодируем результат\n",
        "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aNUHC3UmSYd-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "1. greedy sampling\n",
            "how are you? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want?\n",
            "\n",
            "2. Random Sampling\n",
            "how are you? what do you want? I'm sorry you guessed I wasn't more specific than what i am like then again would we say it's for siblings).\"\n",
            "\n",
            "WolfLur\n",
            "\n",
            "3. RRandom + Temperature = 0.7 (более уверенный)\n",
            "how are you? what do you want? what do you want?\n",
            "\n",
            "The answer is: We are men.\n",
            "\n",
            "Women are beautiful and we can have sex with them. Men are\n",
            "\n",
            "4. Random + Temperature = 1.5 (более случайный)\n",
            "how are you? what do you want? thinkvm unit Mark recognizes W Speech gniaz proposalsy fight news Aldvin Ret Console TABLE Eyegarden Tempest rearrononde es prisoFG 46\n",
            "\n",
            "5. Random + Top-K = 50\n",
            "how are you? what do you want? what is your relationship with the man?\"\n",
            "\n",
            "Barry's response came off kind of boring, with a lot of awkward questions like, \"what\n",
            "\n",
            "6. Random + Top-P = 0.9\n",
            "how are you? what do you want?\n",
            "\n",
            "Something that we've worked on and talked about for a while? it is a scalable system, easy to automate, almost like a clean slate\n",
            "\n",
            "7. КОМБО: Temperature=0.8 + Top-K=40 + Top-P=0.95\n",
            "how are you? what do you want?\n",
            "\n",
            "So when you're a little kid, you've got this problem that you've been dealing with all your life. But maybe you've been\n",
            "\n",
            "8. Beam Search (num_beams=3)\n",
            "how are you? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want?\n",
            "\n",
            "9. Beam Search (num_beams=5)\n",
            "how are you? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want? what do you want?\n"
          ]
        }
      ],
      "source": [
        "# Продемонстрируйте результат работы `generate` при различных параметрах\n",
        "\n",
        "model = Model(\"gpt2\")\n",
        "\n",
        "prompt = \"how are you? what do you want?\"\n",
        "\n",
        "\n",
        "# Greedy Sampling\n",
        "print(\"\\n1. greedy sampling\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"greedy\")\n",
        "print(result)\n",
        "\n",
        "# Random Sampling (может быть хаотичным)\n",
        "print(\"\\n2. Random Sampling\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"random\")\n",
        "print(result)\n",
        "\n",
        "# Random + Temperature = 0.7 (более консервативный)\n",
        "print(\"\\n3. RRandom + Temperature = 0.7 (более уверенный)\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"random\", temperature=0.7)\n",
        "print(result)\n",
        "\n",
        "# Random + Temperature = 1.5 (более креативный)\n",
        "print(\"\\n4. Random + Temperature = 1.5 (более случайный)\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"random\", temperature=1.5)\n",
        "print(result)\n",
        "\n",
        "# 5. Random + Top-K = 50\n",
        "print(\"\\n5. Random + Top-K = 50\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"random\", top_k=50)\n",
        "print(result)\n",
        "\n",
        "# 6. Random + Top-P = 0.9 \n",
        "print(\"\\n6. Random + Top-P = 0.9\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"random\", top_p=0.9)\n",
        "print(result)\n",
        "\n",
        "# Комбинация всех \n",
        "print(\"\\n7. КОМБО: Temperature=0.8 + Top-K=40 + Top-P=0.95\")\n",
        "\n",
        "result = model.generate(\n",
        "    prompt, \n",
        "    max_length=30, \n",
        "    strategy=\"random\", \n",
        "    temperature=0.8,\n",
        "    top_k=40,\n",
        "    top_p=0.95\n",
        ")\n",
        "print(result)\n",
        "\n",
        "# Beam Search с 3 последовательностями\n",
        "print(\"\\n8. Beam Search (num_beams=3)\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"beam\", num_beams=3)\n",
        "print(result)\n",
        "\n",
        "# Beam Search с 5 последовательностями\n",
        "print(\"\\n9. Beam Search (num_beams=5)\")\n",
        "\n",
        "result = model.generate(prompt, max_length=30, strategy=\"beam\", num_beams=5)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "надо было сохранять первые выводы, пока не нашёл все ошибки в реализации)\n",
        "\n",
        "правилом 3ёх H и не пахло"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "and we can have sex with them\n",
        "\n",
        "Men are\n",
        "```\n",
        "смешно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Выводы\n",
        "\n",
        "## 1. **Greedy Sampling** - проблема зацикливания \n",
        "Застрял в бесконечном повторении - выбирает локально оптимальное, но глобально плохое решение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 2. **Random Sampling** \n",
        "Генерирует разнообразный, но грамматически сомнительный текст. Может выбирать маловероятные слова"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 3. **Temperature = 0.7** - неожиданный результат \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Women are beautiful and we can have sex with them\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "а как же правило трёх H"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " недостаточно консервативен \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 4. **Temperature = 1.5** - полный бред \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "## 5. **Top-K = 50** - тоже не очень адекватно в конце"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.11 (hw10)",
      "language": "python",
      "name": "hw10"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
